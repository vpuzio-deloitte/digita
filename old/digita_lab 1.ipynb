{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come usare un notebook Jupyter\n",
    "\n",
    "### Introduzione\n",
    "Un notebook Jupyter è uno strumento interattivo che permette di scrivere e eseguire codice in vari linguaggi di programmazione, come Python, direttamente nel browser. È molto utile per analisi dati, machine learning, e visualizzazione dei dati.\n",
    "\n",
    "### Struttura del notebook\n",
    "Un notebook Jupyter è composto da celle. Ci sono due tipi principali di celle:\n",
    "1. **Celle di codice**: dove si scrive ed esegue il codice.\n",
    "2. **Celle di testo (Markdown)**: dove si può scrivere testo formattato usando Markdown.\n",
    "\n",
    "### Come usare le celle\n",
    "- **Aggiungere una cella**: Clicca sul pulsante `+` nella barra degli strumenti o usa il comando `Insert` dal menu.\n",
    "- **Eseguire una cella**: Seleziona la cella e premi `Shift + Enter` oppure clicca sul pulsante `Run` nella barra degli strumenti.\n",
    "- **Modificare una cella**: Clicca due volte sulla cella di testo o seleziona la cella di codice e inizia a scrivere.\n",
    "\n",
    "Per utilizzare questo notebook, dovrai solo modificare i prompt nelle celle di codice e eseguire le celle per ottenere i risultati desiderati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init e dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.2.7 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (0.2.7)\n",
      "Collecting langchain_aws\n",
      "  Downloading langchain_aws-0.2.11-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: langchain-community in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: langchain_core in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (0.2.43)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.36.10-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.36.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain==0.2.7) (8.2.3)\n",
      "INFO: pip is looking at multiple versions of langchain-aws to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_aws\n",
      "  Downloading langchain_aws-0.2.10-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.9-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.6-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.5-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.4-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-aws to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_aws-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Downloading langchain_aws-0.2.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.34.162-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting langchain_aws\n",
      "  Downloading langchain_aws-0.1.18-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain-community) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain_core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langchain_core) (4.12.2)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from botocore) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from botocore) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (0.26.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.7) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.7) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.7) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.7) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.7) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.7) (3.0.3)\n",
      "Requirement already satisfied: anyio in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/dsantoro_deloitte/work/.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.2.2)\n",
      "Downloading langchain_aws-0.1.18-py3-none-any.whl (83 kB)\n",
      "Downloading boto3-1.34.162-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, langchain_aws\n",
      "Successfully installed boto3-1.34.162 botocore-1.34.162 jmespath-1.0.1 langchain_aws-0.1.18 s3transfer-0.10.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain==0.2.7 langchain_aws langchain-community langchain_core boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm endpoints (to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_deployment=\"gpt4o\",\n",
    "#     openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "#     temperature=0,\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "from botocore.config import Config\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "bedrock_client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name='eu-west-1',\n",
    "        aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        verify=False,  # Disable SSL verification\n",
    "        config=Config(\n",
    "            proxies={'https': None}\n",
    "        )\n",
    "    )\n",
    "\n",
    "llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 2000,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercitazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic prompt\n",
    "\n",
    "In questa esercitazione, l'obiettivo è imparare a strutturare un prompt di base per generare una storia o una filastrocca in rima utilizzando l'llm fornito.\n",
    "Questo sarà la nostra baseline per la valutazione dell'efficacia di prompt successivi.\n",
    "\n",
    "#### Passaggi:\n",
    "\n",
    "1. **Definizione del PromptTemplate**:\n",
    "    - Utilizziamo la classe `PromptTemplate` per definire il template del prompt. Questo template include variabili di input come `protagonist` e `setting` che verranno sostituite con i valori forniti dall'utente.\n",
    "\n",
    "     ```python\n",
    "     prompt_template = PromptTemplate(\n",
    "          input_variables=[\"protagonist\", \"setting\"],\n",
    "          template=\"\"\"\n",
    "                Genera una storia\n",
    "                Protagonista:\n",
    "                {protagonist} \n",
    "\n",
    "                Ambientazione:\n",
    "                {setting}\n",
    "          \"\"\"\n",
    "     )\n",
    "     ```\n",
    "\n",
    "2. **Creazione della catena di elaborazione**:\n",
    "    - Combiniamo il `PromptTemplate` con il modello di linguaggio (`llm`) e un parser di output (`StrOutputParser`) per creare una catena di elaborazione (`chain`). Questa catena prende il prompt generato e lo invia al modello di linguaggio per ottenere una risposta.\n",
    "\n",
    "     ```python\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "     ```\n",
    "\n",
    "3. **Generazione della storia**:\n",
    "    - Definiamo una funzione `generate` che prende in input il protagonista e l'ambientazione, e utilizza la catena di elaborazione per generare una storia.\n",
    "\n",
    "     ```python\n",
    "     def generate(protagonist, setting):\n",
    "          response = chain.invoke({\"protagonist\": protagonist, \"setting\": setting})\n",
    "          return response\n",
    "     ```\n",
    "\n",
    "4. **Esecuzione della funzione**:\n",
    "    - Eseguiamo la funzione `generate` con i valori desiderati per il protagonista e l'ambientazione, e visualizziamo la storia generata.\n",
    "\n",
    "     ```python\n",
    "     response = generate(\"Alice\", \"un bosco\")\n",
    "     print(response)\n",
    "     ```\n",
    "\n",
    "Questa esercitazione mostra come utilizzare i componenti di LangChain per creare un prompt di base e generare contenuti creativi come storie o filastrocche in rima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(protagonist, setting):\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"protagonist\", \"setting\"],\n",
    "        template=\"\"\"\n",
    "            Protagonista:\n",
    "            {protagonist} \n",
    "\n",
    "            Ambientazione:\n",
    "            {setting}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"protagonist\": protagonist, \"setting\": setting})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(\"Alice\", \"un bosco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ecco una possibile storia breve ambientata in un bosco con protagonista Alice:\\n\\nAlice si incamminò nel folto del bosco, le foglie secche scricchiolavano sotto i suoi passi. Aveva bisogno di staccarsi dalla routine quotidiana e trovare un po' di pace in mezzo alla natura. \\n\\nMentre camminava, il suo sguardo venne attratto da una luce soffusa che filtrava tra gli alberi. Incuriosita, si diresse in quella direzione e si ritrovò in una radura illuminata dai raggi del sole. Al centro c'era una vecchia quercia, i cui rami sembravano protendere verso di lei, come per accoglierla.\\n\\nAlice si sedette ai piedi dell'albero, appoggiando la schiena al tronco rugoso. Un senso di tranquillità la avvolse. Chiuse gli occhi, ascoltando il canto degli uccelli e il fruscio delle foglie mosse dal vento. In quel momento, ebbe l'impressione che la quercia stesse sussurrandole qualcosa. \\n\\nAprì gli occhi e sorrise, sentendosi finalmente in pace con se stessa. Il bosco le aveva regalato la serenità di cui aveva bisogno.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Few Shot Prompt\n",
    "\n",
    "In questa esercitazione, l'obiettivo è imparare a creare un prompt \"few-shot\" per generare una storia o una filastrocca in rima di complessità superiore alla precedente.\n",
    "\n",
    "#### Passaggi:\n",
    "\n",
    "1. **Definizione del PromptTemplate**:\n",
    "    - Utilizziamo la classe `PromptTemplate` per definire il template del prompt. Questo template include esempi di input e output per aiutare il modello a comprendere meglio il compito.\n",
    "    - **Few-shot learning**: Il few-shot learning è una tecnica in cui il modello viene addestrato con un numero limitato di esempi (da pochi a poche decine) per ogni compito. Questo approccio è utile quando non si dispone di grandi quantità di dati di addestramento. Per costruire un prompt few-shot, includiamo nel template alcuni esempi di input e output che il modello può utilizzare come riferimento per generare risposte accurate.\n",
    "\n",
    "2. **Creazione della catena di elaborazione**:\n",
    "    - Combiniamo il `PromptTemplate` con il modello di linguaggio (`llm`) e un parser di output (`StrOutputParser`) per creare una catena di elaborazione (`chain`). Questa catena prende il prompt generato e lo invia al modello di linguaggio per ottenere una risposta.\n",
    "\n",
    "3. **Generazione della storia**:\n",
    "    - Definiamo una funzione `generate_few_shot` che prende in input il protagonista e l'ambientazione, e utilizza la catena di elaborazione per generare una storia.\n",
    "\n",
    "\n",
    "4. **Esecuzione della funzione**:\n",
    "    - Eseguiamo la funzione `generate_few_shot` con i valori desiderati per il protagonista e l'ambientazione, e visualizziamo la storia generata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_few_shot(protagonist, setting):\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"protagonist\", \"setting\"],\n",
    "        template=\"\"\"\n",
    "            Protagonista:\n",
    "            {protagonist} \n",
    "\n",
    "            Ambientazione:\n",
    "            {setting}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"protagonist\": protagonist, \"setting\": setting})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guiding avanzato\n",
    "\n",
    "In questa sezione, esploreremo come creare una catena di elaborazione avanzata utilizzando tecniche di \"instruction tuning\" e \"role playing\". Queste tecniche permettono di migliorare la qualità delle risposte generate dal modello di linguaggio, fornendo istruzioni dettagliate e simulando ruoli specifici.\n",
    "\n",
    "***Difficoltà aggiuntiva***: cerchiamo di limitare il numero di parole generate dal modello.\n",
    "\n",
    "#### Passaggi:\n",
    "\n",
    "1. **Definizione del PromptTemplate**:\n",
    "\t- Utilizziamo la classe `PromptTemplate` per definire il template del prompt. Questo template include variabili di input come `protagonist` e `setting`, oltre a istruzioni dettagliate per il modello. L'instruction tuning consiste nel fornire al modello istruzioni chiare e dettagliate su come deve rispondere, mentre il role playing implica la simulazione di ruoli specifici per guidare il modello a generare risposte più contestualizzate.\n",
    "\n",
    "2. **Creazione della catena di elaborazione**:\n",
    "\t- Combiniamo il `PromptTemplate` con il modello di linguaggio (`llm`) e un parser di output (`StrOutputParser`) per creare una catena di elaborazione (`chain`). Questa catena prende il prompt generato e lo invia al modello di linguaggio per ottenere una risposta.\n",
    "\n",
    "3. **Generazione della storia**:\n",
    "\t- Definiamo una funzione `generate_guiding` che prende in input il protagonista e l'ambientazione, e utilizza la catena di elaborazione per generare una storia.\n",
    "\n",
    "4. **Esecuzione della funzione**:\n",
    "\t- Eseguiamo la funzione `generate_guiding` con i valori desiderati per il protagonista e l'ambientazione, e visualizziamo la storia generata.\n",
    "    #### Risultato:\n",
    "\n",
    "    La funzione `generate_guiding` utilizza tecniche di instruction tuning e role playing per generare una storia più dettagliata e contestualizzata. Ecco un esempio di storia generata:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_few_shot(\"Alice\", \"un bosco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_guiding(protagonist, setting):\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"protagonist\", \"setting\"],\n",
    "        template=\"\"\"\n",
    "            Protagonista:\n",
    "            {protagonist} \n",
    "\n",
    "            Ambientazione:\n",
    "            {setting}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"protagonist\": protagonist, \"setting\": setting})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_guiding(\"Alice\", \"un bosco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
