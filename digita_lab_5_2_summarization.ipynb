{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71doM1OUvYg6"
      },
      "source": [
        "## Init e dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bA4gJX0vYg7",
        "outputId": "45760629-5f34-4740-8f16-0e4e7185eed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.2.7 in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_aws in /usr/local/lib/python3.11/dist-packages (0.1.18)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.2.43)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.34.162)\n",
            "Requirement already satisfied: botocore in /usr/local/lib/python3.11/dist-packages (1.34.162)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.7) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.10.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore) (2.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.7) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.7) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.7) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.7) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.7) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.7) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.7) (1.3.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain==0.2.7 langchain_aws langchain-community langchain_core boto3 botocore\n",
        "! pip install PyMuPDF python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5rRHe6TjdpS",
        "outputId": "7521c754-5d64-4077-82c2-c5855c0a1d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pymupdf in /home/vpuzio/.local/lib/python3.10/site-packages (1.25.3)\n",
            "Requirement already satisfied: python-docx in /home/vpuzio/.local/lib/python3.10/site-packages (1.1.2)\n",
            "Requirement already satisfied: pydantic in /home/vpuzio/.local/lib/python3.10/site-packages (2.6.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /home/vpuzio/.local/lib/python3.10/site-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /home/vpuzio/.local/lib/python3.10/site-packages (from python-docx) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/vpuzio/.local/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /home/vpuzio/.local/lib/python3.10/site-packages (from pydantic) (2.16.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install pymupdf python-docx pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qsDG8NVvYg8"
      },
      "source": [
        "### llm endpoints (to edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4IQaOfDvYg9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S4PibKiWvYg-"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "from langchain_aws import ChatBedrock\n",
        "from botocore.config import Config\n",
        "import urllib3\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from google.colab import userdata\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "bedrock_client = boto3.client(\n",
        "        service_name='bedrock-runtime',\n",
        "        region_name='eu-west-1',\n",
        "        aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "        verify=False,  # Disable SSL verification\n",
        "        config=Config(\n",
        "            proxies={'https': None}\n",
        "        )\n",
        "    )\n",
        "\n",
        "llm = ChatBedrock(\n",
        "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "        client=bedrock_client,\n",
        "        model_kwargs={\n",
        "            \"temperature\": 0,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RHhNQc9njdpT"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def upload_file():\n",
        "  uploaded = files.upload()\n",
        "  # Una volta eseguita questa cella, Colab ti chiederà di selezionare il file dal tuo PC.\n",
        "\n",
        "  # 2) Identifica il nome del file caricato\n",
        "  for file_name in uploaded.keys():\n",
        "      file_path = file_name\n",
        "\n",
        "  return file_path\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "def upload_and_read_pdf():\n",
        "    file_path = upload_file()\n",
        "\n",
        "    return read_pdf(file_path)\n",
        "\n",
        "def upload_and_read_docx():\n",
        "    file_path = upload_file()\n",
        "\n",
        "    return read_docx(file_path)\n",
        "\n",
        "def upload_and_read_txt():\n",
        "    file_path = upload_file()\n",
        "\n",
        "    return read_txt(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SGpZy-BWvYhA"
      },
      "outputs": [],
      "source": [
        "def generate(text):\n",
        "\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"protagonist\", \"setting\"],\n",
        "        template=\"\"\"\n",
        "            Riassumi il seguente testo:\n",
        "            {text}\n",
        "\n",
        "            REGOLE\n",
        "            - scrivi una breve intro per spiegare il contesto\n",
        "            - riassumi i punti principali in una bullet list\n",
        "            - trai le tue conclusioni sull'argomento\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    response = chain.invoke({\"text\": text})\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X-oF-QT1vYhB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ce02074b-6495-4403-9576-a62c91d34e15"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1643843e-3bff-4de8-bf47-79454d95f9e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1643843e-3bff-4de8-bf47-79454d95f9e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RomeCall_Paper_web.pdf to RomeCall_Paper_web (1).pdf\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = upload_and_read_pdf()\n",
        "\n",
        "response = generate(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oGxg_16ivYhB",
        "outputId": "2215ec81-0ae8-4b40-b073-3d8aa82f1007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Riassunto:\n\nIntroduzione\nIl testo presenta la \"Rome Call for AI Ethics\", un documento firmato nel 2020 per promuovere un approccio etico all'intelligenza artificiale. L'obiettivo è di creare un futuro in cui l'innovazione digitale e il progresso tecnologico mettano l'essere umano al centro.\n\nPunti principali:\n- L'IA offre grandi potenzialità per migliorare il benessere sociale e personale, ma deve essere sviluppata nel rispetto della dignità umana e senza avere come unico scopo il profitto.\n- I firmatari si impegnano a promuovere uno sviluppo dell'IA al servizio dell'intera umanità, che non discrimini nessuno e che sia attento all'ambiente.\n- Vengono definiti 6 principi etici fondamentali per l'IA: trasparenza, inclusione, responsabilità, imparzialità, affidabilità, sicurezza e privacy.\n- Viene sottolineata l'importanza dell'istruzione e della formazione per sensibilizzare sulle opportunità e sui rischi dell'IA.\n- Sono necessarie nuove forme di regolamentazione per promuovere la trasparenza e il rispetto dei principi etici, soprattutto per le tecnologie ad alto rischio di impatto sui diritti umani.\n\nConclusioni\nIl documento rappresenta un importante passo avanti nella definizione di un quadro etico per lo sviluppo dell'intelligenza artificiale, con l'obiettivo di garantire che essa serva l'intera umanità e il pianeta, nel rispetto della dignità e dei diritti di ogni essere umano. L'impegno congiunto di istituzioni, aziende e società civile è fondamentale per realizzare questa visione."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(f\"### Riassunto:\\n\\n{response}\"))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "I4DJ5X2AjdpV",
        "outputId": "2efa5973-f1d9-41c8-a64e-85c336a60731"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b2944be7-a151-4a2f-b945-da3e9a5d7f1f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b2944be7-a151-4a2f-b945-da3e9a5d7f1f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving football_league.docx to football_league.docx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Riassunto:\n\nIntroduzione\nIl testo presenta la \"Rome Call for AI Ethics\", un documento firmato nel 2020 per promuovere un approccio etico all'intelligenza artificiale. L'obiettivo è di creare un futuro in cui l'innovazione digitale e il progresso tecnologico mettano l'essere umano al centro.\n\nPunti principali:\n- L'IA offre grandi potenzialità per migliorare il benessere sociale e personale, ma deve essere sviluppata nel rispetto della dignità umana e senza avere come unico scopo il profitto.\n- I firmatari si impegnano a promuovere uno sviluppo dell'IA al servizio dell'intera umanità, che non discrimini nessuno e che sia attento all'ambiente.\n- Vengono definiti 6 principi etici fondamentali per l'IA: trasparenza, inclusione, responsabilità, imparzialità, affidabilità, sicurezza e privacy.\n- Viene sottolineata l'importanza dell'istruzione e della formazione per sensibilizzare sulle opportunità e sui rischi dell'IA.\n- Sono necessarie nuove forme di regolamentazione per promuovere la trasparenza e il rispetto dei principi etici, soprattutto per le tecnologie ad alto rischio di impatto sui diritti umani.\n\nConclusioni\nIl documento rappresenta un importante passo avanti nella definizione di un quadro etico per lo sviluppo dell'intelligenza artificiale, con l'obiettivo di garantire che essa serva l'intera umanità e il pianeta, nel rispetto della dignità e dei diritti di ogni essere umano. L'impegno congiunto di istituzioni, aziende e società civile è fondamentale per realizzare questa visione."
          },
          "metadata": {}
        }
      ],
      "source": [
        "text = upload_and_read_docx()\n",
        "\n",
        "summary = generate(text)\n",
        "\n",
        "display(Markdown(f\"### Riassunto:\\n\\n{response}\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = upload_and_read_txt()\n",
        "\n",
        "summary = generate(text)\n",
        "\n",
        "display(Markdown(f\"### Riassunto:\\n\\n{response}\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "qDydCxn1pQ02",
        "outputId": "57e5b698-e3d0-42ab-9e7e-176e0ee70038"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-92926f4a-398d-4d95-bcf1-60b12e1e371e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-92926f4a-398d-4d95-bcf1-60b12e1e371e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving commedia.txt to commedia.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Input is too long for requested model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_aws/llms/bedrock.py\u001b[0m in \u001b[0;36m_prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Input is too long for requested model.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0f18976e47be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_and_read_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"### Riassunto:\\n\\n{response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-20d2da531360>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_template\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mStrOutputParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2877\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2879\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2880\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2881\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         return cast(\n\u001b[1;32m    276\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    776\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         flattened_outputs = [\n\u001b[1;32m    636\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 results.append(\n\u001b[0;32m--> 624\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    625\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    847\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_aws/chat_models/bedrock.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stop_sequences\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             completion, tool_calls, llm_output = self._prepare_input_and_invoke(\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_aws/llms/bedrock.py\u001b[0m in \u001b[0;36m_prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by bedrock service: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Input is too long for requested model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100000, chunk_overlap=500)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "print(len(chunks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li6b-Yt1pcZe",
        "outputId": "da8c4ffa-6133-4abb-8b44-2187c22684e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "for chunk in chunks:\n",
        "    response = generate(chunk)\n",
        "    summaries.append(response)\n"
      ],
      "metadata": {
        "id": "pEOlyw2rpld6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "summary = \"\\n\".join(summaries)\n",
        "\n",
        "display(Markdown(f\"### Riassunto:\\n\\n{summary}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nePjszMWqdFb",
        "outputId": "9de8b80a-4397-4658-908f-2c82733fe026"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Riassunto:\n\nContesto:\nDante e Virgilio proseguono il loro viaggio nell'Inferno, arrivando alla quarta bolgia del settimo cerchio, dove sono puniti i simoniaci. Qui incontrano alcuni personaggi famosi, tra cui il papa Bonifacio VIII, che li rimprovera credendoli altri papi.\n\nPunti principali:\n- Dante e Virgilio arrivano alla quarta bolgia, dove sono puniti i simoniaci, coloro che hanno fatto commercio delle cose sacre.\n- Incontrano il papa Bonifacio VIII, che li rimprovera credendoli altri papi, e Dante lo rimprovera a sua volta per la sua avidità e corruzione.\n- Dante critica duramente la corruzione del clero, in particolare dei papi, che hanno anteposto l'avidità di denaro e potere al loro dovere spirituale.\n- Virgilio approva le parole di Dante, segno che condivide la sua indignazione morale.\n\nConclusioni:\nIl canto XX dell'Inferno rappresenta una forte denuncia di Dante contro la corruzione del clero, in particolare dei papi, che hanno tradito il loro ruolo spirituale per inseguire il potere e la ricchezza. Questa critica morale è espressa attraverso l'incontro con Bonifacio VIII, che viene duramente rimproverato da Dante per i suoi peccati di simonia e avidità. Il poema dantesco si configura così come una severa condanna della degenerazione della Chiesa del suo tempo.\nContesto:\nNel Canto IV del Purgatorio, Dante e Virgilio continuano il loro viaggio verso la vetta del Purgatorio. Incontrano un gruppo di anime che stanno salendo lentamente il monte, tra cui riconoscono Manfredi, nipote dell'imperatrice Costanza. Manfredi racconta la sua storia e chiede a Dante di riferire alla sua figlia Costanza come è morto e che, nonostante i suoi peccati, la bontà divina lo ha accolto.\n\nPunti principali:\n\n- Dante e Virgilio incontrano un gruppo di anime che stanno salendo lentamente il monte del Purgatorio.\n- Tra loro riconoscono Manfredi, nipote dell'imperatrice Costanza.\n- Manfredi racconta la sua storia: è morto in battaglia, ma nonostante i suoi peccati, la bontà divina lo ha accolto.\n- Manfredi chiede a Dante di riferire alla sua figlia Costanza come è morto e che la bontà di Dio lo ha perdonato.\n- Manfredi spiega che, nonostante la scomunica della Chiesa, la speranza nella misericordia divina non si perde mai.\n\nConclusioni:\n\nQuesto canto mette in luce il tema della misericordia divina e della possibilità di redenzione anche per i peccatori più gravi. Manfredi, nonostante i suoi peccati e la scomunica, è stato accolto dalla bontà di Dio grazie alla sua speranza e pentimento finale. Questo episodio sottolinea l'importanza della fede e della fiducia nella misericordia divina, anche di fronte ai più gravi errori.\n\nPurgatorio: Canto IV\n\n  Quando per dilettanze o ver per doglie,\nche alcuna virtù nostra comprenda,\nl'anima bene ad essa si raccoglie,\n  par ch'a nulla potenza più intenda;\ne questo è contra quello error che crede\nch'un'anima sovr'altra in noi s'accenda.\n  E però, quando s'ode cosa o vede\nche tegna forte a sé l'anima volta,\nvassene 'l tempo e l'uom non se n'avvede;\n  ch'altra potenza è quella che l'ascolta,\ne altra è quella c'ha l'anima intera:\nquesta è quasi legata, e quella è sciolta.\n  Di ciò ebb'io esperienza vera,\nudendo quello spirto e ammirando;\nché ben cinquanta gradi salito era\n  lo sole, e io non m'era accorto, quando\nvenimmo ove quell'anime ad una\ngridaro a noi: \"Qui è vostro dimando\".\n  Maggiore aperta molte volte impruna\ncon una forcatella di sue spine\nl'uom de la villa, quando l'uva imbruna,\n  che non era la calla onde salìne\nlo duca mio, e io appresso, soli,\ncome da noi la schiera si partìne.\n  Levati su\", dicea, \"frate, e vedi\nche 'l sole è già a terza rag giunto;\ne tra esser vivi e morti non vedi\n  altro, dì', che questo?\" Io m'era appunto\nfra 'l fin de l'una e 'l principio de l'altra,\nsì com'io dissi, fatto quasi eterno,\n  e non v'era mestier molta favella;\ne com'io giunsi in su l'orlo soprano,\nattento si fermò com'uom ch'ascolta;\n  perché ne l'aere, dolce sanza vento,\nvenìa una voce sola a me, dicendo:\n\"Asperges me\" sì dolcemente, che\n  non so dir com'udilla, e non ridendo.\nPoi cominciò: \"Per entro i gradi suoi\ndiscende mai alcun di voi mortali\n  per far di noi poi special rivela?\nQuesta gente che preme a noi è molta,\ne a riguardar più altre le console\".\n  Quinci infer'io, congetturando, che\nforte spiacesse a costor l'esser mòrti,\npoi che parea che a nulla fosser vòlti.\n  Ond'io: \"Maestro, tra cotesti incerti\nforse colui che vinto fa ciascuno,\nquanto in sua vita per l'altro si spese,\n  volentier tornerebbe al suo paese;\ne io son un che piango, e un che canta,\nper l'altrui colpa sì come per la mia\".\n  Non altrimenti stupido si stanta\nlo montanaro, e rimirando ammuta,\nquando rozzo e salvatico s'incianta,\n  che ciascun'ombra fece in sua paruta;\nma poi che furon di stupore scarche,\nlo qual ne li alti cuor tosto s'attuta,\n  \"Beato te, che de le nostre marche\",\nricominciò colei che pria m'inchiese,\n\"per morir meglio, esperienza imbarche!\n  La gente che non vien con noi, offese\ndi ciò per che già Cesar, trionfando,\n\"Regina\" contra sé chiamar s'intese:\n  però si parton \"Soddoma\" gridando,\nrimproverando a sé, com'hai udito,\ne aiutan l'arsura vergognando.\n  Nostro peccato fu ermafrodito;\nma perché non servammo umana legge,\nseguendo come bestie l'appetito,\n  in obbrobrio di noi, per noi si legge,\nquando partinci, il nome di colei\nche s'imbestiò ne le 'mbestiate schegge.\n  Or sai nostro peccato e quel ch'ei fue;\nse forse a nome vuo' saper chi semo,\ntempo non è di dire, e non saprei.\n  Questa mia compagna, che si sface\ne disfà in suo cammino, sùbita,\nè Lia; e va movendo intorno le belle mani\n  a farsi una ghirlanda; e Rachele\nmai non si smaga dal suo mirar davante,\ncontempl ando, quanto questo ciel ne 'nvera.\n  E l'altra, che le chiome s'avvalora,\nsiede e compone il suo sembiante. Raele\nè de' suoi belli occhi veder vaga\n  com'io de la sua bocca, per dirla;\ne la Lia, l'attiva, per piacere\na sé più che ad altrui, sua mano snella.\n  Io son, tra esse, nove, e l'altre sono\nlo nome di colei di cui tu fai\nmenzione nel principio di questo canto.\n  Qui si mostra ben che noi siam gente\nnova, e che 'l ciel n'ha fatta grazia grande,\npurch'al Maestro piaccia che s'ascende\".\n  Così parlavan gl'incontrati spirti;\nma poco prima, sì com'io seguia,\nnon so che bianco, e di sotto a quel bianco\n  venìa gridando: \"Dolce Maria!\" e\ndisse: \"O anime, in cui lo foco spira\nancor virtù di quel che par distrutto;\n  non v'arrestate, ma studiate il passo,\nmentre che l'occidente non s'annera\".\nE io, che del color mi fui accorto,\n  dissi: \"Maestro, chi son quelle genti\nche par nel muover de li occhi si vinte?\".\nEd elli a me: \"Non ti maravigliar\n  se ancor li affatica 'l voto antico,\nne l'eserc\nContesto:\nNel Canto XXII del Purgatorio, Dante e Virgilio incontrano l'ombra del poeta Stazio, che spiega loro come sia giunto in Purgatorio. Stazio racconta la sua storia e il suo percorso di redenzione, rivelando il suo amore per l'opera di Virgilio, che lo ha ispirato nella sua attività poetica.\n\nPunti principali:\n- Stazio rivela di essere stato un poeta pagano, ma di essersi convertito al cristianesimo, pur senza avere ancora la fede completa.\n- Stazio racconta di essere stato influenzato dall'opera di Virgilio, in particolare dall'Eneide, che lo ha ispirato nella sua attività poetica.\n- Stazio spiega che il suo ritardo nella conversione è dovuto all'avarizia, che lo ha tenuto lontano dalla vera fede per molto tempo.\n- Stazio esprime la sua ammirazione per Virgilio e il suo desiderio di poterlo abbracciare, ma Virgilio lo ferma, ricordandogli che essendo un'ombra non può essere toccato.\n- Stazio e Virgilio si scambiano parole di affetto e stima, riconoscendo il loro legame artistico e spirituale.\n\nConclusioni:\nIl Canto XXII del Purgatorio mette in luce il tema della redenzione e della conversione, attraverso la figura di Stazio. La sua storia rappresenta il percorso di molti intellettuali e artisti del passato, che pur essendo pagani, hanno trovato ispirazione e spunto di riflessione nell'opera di Virgilio, considerato il \"padre\" della poesia latina. Inoltre, il dialogo tra Stazio e Virgilio sottolinea l'importanza della trasmissione della cultura e della conoscenza, che avviene attraverso il legame tra maestro e allievo.\nContesto:\nDante continua il suo viaggio nel Paradiso, accompagnato da Beatrice. In questo canto, Beatrice risponde alle domande di Dante riguardanti la giustizia divina e la predestinazione.\n\nPunti principali:\n- Beatrice spiega che le anime beate sono felici nella loro condizione attuale, perché la loro volontà è in perfetta armonia con la volontà di Dio.\n- Discute il tema della libertà di volontà, affermando che essa è il dono più grande che Dio ha fatto all'uomo.\n- Chiarisce il concetto di voto e di come esso possa essere sostituito con altre opere buone, purché ciò avvenga con il consenso della Chiesa.\n- Spiega come la giustizia divina non sia sempre comprensibile all'intelletto umano, ma che essa è sempre perfetta.\n- Fornisce un excursus storico sull'Impero Romano e sul suo ruolo provvidenziale nella storia dell'umanità.\n- Introduce il personaggio di Romeo, un uomo giusto e umile, che fu mal ricompensato dai suoi signori.\n\nConclusioni:\nQuesto canto approfondisce temi teologici e filosofici fondamentali, come la libertà di volontà, la giustizia divina e il ruolo della Chiesa. Beatrice dimostra una profonda conoscenza di questi argomenti e li spiega con chiarezza a Dante, guidandolo verso una comprensione più profonda del disegno divino.\nContesto:\nDante continua il suo viaggio nel Paradiso, accompagnato da Beatrice. In questo canto, Dante incontra un'anima beata che lo interroga sulla sua fede, per poi lodare la sua risposta.\n\nPunti principali:\n- Un'anima beata interroga Dante sulla sua fede, chiedendogli di definirla e spiegarne le basi.\n- Dante risponde citando la definizione di fede data da San Paolo, come \"sostanza di cose sperate e argomento di cose non parventi\".\n- Dante spiega che la fede si fonda sulla credenza in ciò che non è visibile, ma che è testimoniato dalle Sacre Scritture e dai miracoli.\n- L'anima beata approva la risposta di Dante, lodandolo per aver compreso correttamente il concetto di fede.\n- Dante viene quindi benedetto tre volte dall'anima beata, in segno di approvazione.\n\nConclusioni:\nQuesto canto sottolinea l'importanza della fede per Dante, che la definisce come fondamento della speranza cristiana e della conoscenza di Dio. La risposta di Dante dimostra la sua profonda comprensione della dottrina cristiana, che gli viene riconosciuta dall'anima beata. Questo episodio evidenzia come la fede sia centrale nel viaggio di Dante verso la salvezza.\nContesto:\nQuesto testo fa parte del Paradiso, la terza cantica della Divina Commedia di Dante Alighieri. Qui Dante descrive la sua visione del Paradiso, guidato da Beatrice, e il suo incontro con figure e simboli sacri.\n\nPunti principali:\n- Dante esprime il desiderio di poter raccontare adeguatamente la sua esperienza mistica nel Paradiso, dopo essere stato a lungo \"macro\" (magro) per la fatica di comporre il poema sacro.\n- Dante vede l'anima di San Giacomo, apostolo di Gesù, che lo accoglie e lo interroga sulla speranza.\n- Beatrice spiega a Dante la gerarchia degli angeli e la creazione dell'universo, sottolineando l'importanza della grazia divina.\n- Dante viene guidato da San Bernardo in una visione della Vergine Maria, che intercede per lui presso Dio.\n- Dante raggiunge la visione diretta di Dio, sperimentando l'ineffabile mistero della Trinità.\n\nConclusioni:\nIl testo rappresenta il culmine dell'esperienza mistica di Dante nel Paradiso, in cui egli riesce a contemplare direttamente la luce divina. Questo episodio simboleggia l'apice della conoscenza e dell'unione con Dio, raggiungibile solo attraverso la grazia e la fede. Il linguaggio poetico di Dante si eleva qui a vette di straordinaria intensità e profondità teologica."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TODO: implementare ricerca"
      ],
      "metadata": {
        "id": "7o5ntc0uq7RX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}