{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArHkrx3M40ie"
      },
      "source": [
        "## Init e dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVdUPdJg40ig"
      },
      "outputs": [],
      "source": [
        "! pip install langchain==0.2.7 langchain_aws langchain-community langchain_core boto3 botocore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM endpoints"
      ],
      "metadata": {
        "id": "XJm1Qzvg7c7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l7ONprNM40ii"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import os\n",
        "from langchain_aws import ChatBedrock\n",
        "from botocore.config import Config\n",
        "import urllib3\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "bedrock_client = boto3.client(\n",
        "        service_name='bedrock-runtime',\n",
        "        region_name='eu-west-1',\n",
        "        aws_access_key_id=userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "        aws_secret_access_key=userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "        verify=False,  # Disable SSL verification\n",
        "        config=Config(\n",
        "            proxies={'https': None}\n",
        "        )\n",
        "    )\n",
        "\n",
        "llm = ChatBedrock(\n",
        "        model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "        client=bedrock_client,\n",
        "        model_kwargs={\n",
        "            \"temperature\": 0,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFVrrQq40ii"
      },
      "source": [
        "## While True\n",
        "\n",
        "Il costrutto **while True** è una struttura di controllo utilizzata nei linguaggi di programmazione per creare loop (cicli) infiniti. In Python, un ciclo while esegue ripetutamente un blocco di codice finché la condizione specificata è vera.\n",
        "\n",
        "Nel caso di while True, la condizione è **sempre vera**, quindi il ciclo continua all'infinito finché non viene interrotto manualmente.\n",
        "\n",
        "In questo caso, il ciclo viene utilizzato per richiedere all'utente (tramite comando **input**) un messaggio da girare alla LLM.\n",
        "\n",
        "Nel momento in cui l'utente inserisce una parola chiave, si usa il **break** per uscire dal ciclo infinito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1EWlYra40ii"
      },
      "outputs": [],
      "source": [
        "print(\"Chatbot per l'ordine - Digita 'esci' per uscire\")\n",
        "while True:\n",
        "    comando = input(\"Scrivi 'esci' per terminare: \")\n",
        "    print(f\"hai scritto {comando}\")\n",
        "    if comando.lower() == \"esci\":\n",
        "        print(\"Ciclo terminato.\")\n",
        "        break\n",
        "\n",
        "print(\"Fuori dal ciclo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "\n",
        "La **ConversationBufferMemory** è un meccanismo fornito da LangChain che tiene traccia delle interazioni passate tra utente e assistente (cioè la cronologia della chat). In pratica, **memorizza** gli ultimi **input** (domande dell’utente) e **output** (risposte del bot).\n",
        "\n",
        "Questo è utile perché consente al modello di **mantenere il contesto** delle conversazioni precedenti, dando risposte più coerenti e pertinenti mano a mano che la chat procede."
      ],
      "metadata": {
        "id": "-GkeOSOy7qek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "system_message = \"\"\"\n",
        "        Sei un assistente virtuale\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "6jX5d2IEI9BA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chatbot - Digita 'esci' per uscire\")\n",
        "\n",
        "def chat_loop():\n",
        "  while True:\n",
        "      user_input = input(\"================> Tu: \")\n",
        "      if user_input.lower() == \"esci\":\n",
        "          break\n",
        "\n",
        "      # Recupero della conversazione passata\n",
        "      chat_history = memory.load_memory_variables({}).get(\"history\", [])\n",
        "\n",
        "      # Formattazione del prompt\n",
        "      messages = [SystemMessage(content=system_message)]\n",
        "      for msg in chat_history:\n",
        "        print(f\"HISTORY: {msg}\")\n",
        "        messages.append(msg)\n",
        "\n",
        "      messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "      # Ottenere la risposta del modello\n",
        "      prompt = ChatPromptTemplate.from_messages(messages)\n",
        "      chain = prompt | llm | output_parser\n",
        "      response = chain.invoke({})\n",
        "\n",
        "      # Salvare il contesto nella memoria\n",
        "      memory.save_context({\"input\": user_input}, {\"output\": response})\n",
        "\n",
        "      print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luv0h9ew74z_",
        "outputId": "cf2ab327-cd07-4a11-96c1-9b730421cb29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot - Digita 'esci' per uscire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_loop()"
      ],
      "metadata": {
        "id": "9CoQErrElct_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPJISPql40ij"
      },
      "source": [
        "## Esercitazione\n",
        "\n",
        "### Modificare il chat loop per implementare un assistente AI interattivo\n",
        "\n",
        "#### Esempi\n",
        "\n",
        "#### - Gestione Ordini (E-commerce)\n",
        "Obiettivo: Permettere all’utente di visualizzare, modificare o tracciare i propri ordini su un e-commerce.\n",
        "Esempio:\n",
        "\n",
        "    Utente: «Voglio controllare lo stato del mio ordine #12345.»\n",
        "    Assistente: «L’ordine #12345 è in spedizione. Arriverà lunedì.»\n",
        "    Utente: «Posso aggiungere un nuovo articolo prima che venga spedito?»\n",
        "    Assistente: «La spedizione è già stata avviata, quindi non è possibile modificare quest’ordine. Vuoi creare un nuovo ordine?»\n",
        "\n",
        "#### - Guida Turistica\n",
        "Obiettivo: Fornire consigli sui luoghi da visitare, itinerari di viaggio o informazioni pratiche (orari di apertura, costi, trasporti).\n",
        "Esempio:\n",
        "\n",
        "    Utente: «Cosa posso visitare in Toscana in due giorni?»\n",
        "    Assistente: «Puoi dedicare il primo giorno a Firenze, visitando il Duomo, gli Uffizi e Ponte Vecchio. Il secondo giorno ti consiglio Pisa, per ammirare la Torre Pendente, e Lucca con le sue mura storiche.»\n",
        "    Utente: «Quanto costa l’ingresso agli Uffizi?»\n",
        "    Assistente: «Il biglietto standard costa circa 20€, ma ci sono riduzioni per studenti e over 65.»\n",
        "\n",
        "#### - Servizio di Prenotazione\n",
        "Obiettivo: Aiutare l’utente a prenotare un ristorante, un volo o un albergo, gestendo disponibilità e preferenze.\n",
        "Esempio:\n",
        "\n",
        "    Utente: «Vorrei prenotare un tavolo per 4 persone in un ristorante vegetariano, stasera alle 20.»\n",
        "    Assistente: «Ho trovato tre ristoranti vegetariani nella tua zona con disponibilità per le 20. Ecco i nomi e le valutazioni: ... Vuoi procedere con la prenotazione?»\n",
        "    Utente: «Sì, prenota il ristorante La Ghirlanda.»\n",
        "    Assistente: «Fatto! Ho prenotato un tavolo per 4 alle 20. Ti ho inviato un’e-mail di conferma.»"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}